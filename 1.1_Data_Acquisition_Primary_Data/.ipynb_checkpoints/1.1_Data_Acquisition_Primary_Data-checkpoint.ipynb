{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Assets/1.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Contents:`\n",
    "\n",
    "- [Load Libraries](#load)\n",
    "- [Metallica.com](#web)\n",
    "\t- [Obtaining page URLs](#page) \n",
    "\t- [Obtaining gig URLs](#gig) \n",
    "- [Web scraping from Metallica.com](#scrape) \n",
    "    - [Helper Functions](#help) \n",
    "\t- [Initiating the gig scrape](#script)\n",
    "- [Save final data](#scrape)        \n",
    "- [Next Steps](#next)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "# `Load Libraries`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"web\"></a>\n",
    "# `Metallica.com`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 99 pages that cover all gigs from 2018-1982. Each of these pages has its own unique URL. Let's call these 'page URLs'. \n",
    "    \n",
    "On each of these page URL's there are several 'more info' buttons which have their own URL link to a gig. Let's call these 'gig URL's'. \n",
    "\n",
    "Therefore it seems that the most efficient strategy is to first scrape the 99 page URLs and then for each page URL scrape the 21 gig URLS that are contained within it. The ultimate aim is to create a list of *all* gig urls - which should be about 2100 in total. \n",
    "\n",
    "Once we have all gig URLS we can run a script to scrape all the relevant gig data from them sequentially in a browser.\n",
    "\n",
    "The diagram below should help explain this process a little more\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Assets/urloverview.png\" style=\"width: 800px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"page\"></a>\n",
    "## `Obtaining Page URL's`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for page URL's \n",
    "page_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise driver \n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "#Get URL's for each page\n",
    "driver.get('https://www.metallica.com/tour/past/?sz=21&start=0')    \n",
    "page = driver.find_element_by_class_name('page-next')\n",
    "page_urls.append(page.get_attribute('href'))     \n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        driver.get(page_urls[-1])\n",
    "        page = driver.find_element_by_class_name('page-next')\n",
    "        page_urls.append(page.get_attribute('href'))    \n",
    "    except:\n",
    "        break\n",
    "\n",
    "#add the first page back in\n",
    "page_urls.insert(0,'https://www.metallica.com/tour/past/?sz=21&start=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Page URLs\n",
       "0  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "1  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "2  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "3  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "4  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "5  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "6  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "7  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "8  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "9  https://www.metallica.com/tour/past/?sz=21&sta..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks like it worked\n",
    "page_urls_df = pd.DataFrame(page_urls)\n",
    "page_urls_df.columns =['Page URLs']\n",
    "page_urls_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save these just in case we need them later\n",
    "page_urls_df.to_csv('./Page_Gig_URLs/Metallica_Page_URLS.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gig\"></a>\n",
    "## `Obtaining Gig URL's`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for gig URL's \n",
    "gig_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the gig URL's on each page URL\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "for i in page_urls:\n",
    "    driver.get(i) \n",
    "    all_gigs = driver.find_elements_by_class_name('show')\n",
    "\n",
    "    for each in all_gigs:\n",
    "        show_id = each.get_attribute('data-show-id')\n",
    "        try:\n",
    "            gig_url.append(f'https://www.metallica.com/events/{show_id}.html')\n",
    "        except:\n",
    "            gig_url.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.metallica.com/events/event-36831.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.metallica.com/events/event-36830.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.metallica.com/events/event-36829.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.metallica.com/events/event-36828.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.metallica.com/events/event-36827.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  https://www.metallica.com/events/event-36831.html\n",
       "1  https://www.metallica.com/events/event-36830.html\n",
       "2  https://www.metallica.com/events/event-36829.html\n",
       "3  https://www.metallica.com/events/event-36828.html\n",
       "4  https://www.metallica.com/events/event-36827.html"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks like it's worked\n",
    "gig_urls_df = pd.DataFrame(gig_url)\n",
    "gig_urls_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, let's save it to a CSV just in case we need it later on\n",
    "gig_urls_df.to_csv('./Page_Gig_URLs/Metallica_Gig_URLS.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scrape\"></a>\n",
    "# `Web Scraping from Metallica.com`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logical way to organise the scraped data will be to put each field into its own list - as below. Once I've found how all the elements are tagged on the gig page, I can extract the data I need and append to the relevant list. Once that has been done I can merge the lists and create the initial dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Date \n",
    "    - Venue \n",
    "    - City_Country\n",
    "    - Tour \n",
    "    - Set\n",
    "    - Encores\n",
    "    - Number_of_Encores \n",
    "    - Set_Length \n",
    "    - URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"help\"></a>\n",
    "## `Helper Functions`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following few functions will help to clean the scrape at source and add a few interesting features as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes numbers\n",
    "def removenumbers(x):\n",
    "    return re.sub(\"\\d\",\"\",x)\n",
    "\n",
    "#returns location of gig\n",
    "def location(x):\n",
    "    return x.split('\\n')[0]\n",
    "\n",
    "#returns gig venue\n",
    "def venue(x):\n",
    "    venue_date = x.split('\\n')[1]\n",
    "    return venue_date.split(' / ')[0]\n",
    "\n",
    "#returns gig date\n",
    "def date(x):\n",
    "    venue_date = x.split('\\n')[1]\n",
    "    return venue_date.split(' / ')[1]\n",
    "\n",
    "#returns tour name\n",
    "def tour_clean(x):\n",
    "    return x.split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts set \n",
    "def set_extractor(x):\n",
    "    clean = removenumbers(x)\n",
    "    output = clean.split('ENCORE')[0]\n",
    "    return output\n",
    "\n",
    "#cleans set text\n",
    "def set_clean(x):\n",
    "    new = x.split('\\n\\n')\n",
    "    return [i.replace('\\n','') for i in new][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts encore \n",
    "def encore_extractor(x):\n",
    "    clean = removenumbers(x)\n",
    "    output = clean.split('ENCORE')[1:]\n",
    "    return str(output)\n",
    "\n",
    "#cleans encore text\n",
    "def encore_clean(x):\n",
    "    clean = x.replace(' #','').replace(\"['\",'').replace(\"']\",'').replace(\"\\\\n', '\",'').split('\\\\n\\\\n')\n",
    "    return clean[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts tour\n",
    "def tour_finder(x):\n",
    "    try:\n",
    "        one = x.split('TOUR NAME')[1]\n",
    "        return one.split('\\n')[1]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts support acts\n",
    "def other_acts(x):\n",
    "    try:\n",
    "        one = x.split('OTHER ACTS')[1]\n",
    "        return one.split('\\n')[1]\n",
    "    except:\n",
    "        return np.nan\n",
    "       \n",
    "#if other acts are several, this splits them and appends the to a list\n",
    "def other_act_split(x):\n",
    "    try:\n",
    "        return x.split(',')\n",
    "    except:\n",
    "        return list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 2078 gigs to scrape\n",
    "len(gig_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"script\"></a>\n",
    "## `Initiating the gig scrape`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up empty lists\n",
    "Date = []\n",
    "Venue = []\n",
    "City_Country = []\n",
    "Tour = []\n",
    "Set = []\n",
    "Encores = []\n",
    "Encores_Count = []\n",
    "Set_Length = []\n",
    "Other_Acts = []\n",
    "URL = []\n",
    "\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "for gig in tqdm_notebook(gig_url):\n",
    "    driver.get(gig)\n",
    "\n",
    "#=== append URL, location and date data - some of the cleaning functions above built in so less to clean afterwards    \n",
    "#=== URL appending will be good for spot checks to see if any NaN's are geniune\n",
    "\n",
    "    URL.append(gig)\n",
    "    item_location_date = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[1]/div/div')\n",
    "    try:\n",
    "        City_Country.append(location(item_location_date.text).title())\n",
    "    except:\n",
    "        City_Country.append(np.nan)\n",
    "    try:\n",
    "        Date.append(date(item_location_date.text).title())\n",
    "    except:\n",
    "        Date.append(np.nan)\n",
    "    try:\n",
    "        Venue.append(venue(item_location_date.text).title())\n",
    "    except:\n",
    "        Venue.append(np.nan)\n",
    "\n",
    "#=== append Tour name data\n",
    "    item_content = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]')\n",
    "    try:\n",
    "        Tour.append(tour_finder(item_content.text))\n",
    "    except:\n",
    "        Tour.append(np.nan) \n",
    "\n",
    "#=== append other act data\n",
    "    item_content = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]')\n",
    "    try:\n",
    "        Other_Acts.append([i.strip() for i in other_act_split(other_acts(item_content.text))])\n",
    "    except:\n",
    "        Other_Acts.append(np.nan)   \n",
    "        \n",
    "\n",
    "#=== append Set and Encore data. There are several places on the page this appears across the 2000 gigs so need multiple elements\n",
    "    try:\n",
    "        try:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div[2]')\n",
    "            try:\n",
    "                Set.append(set_clean(set_extractor(item_set.text)))\n",
    "            except:\n",
    "                Set.append(np.nan)\n",
    "\n",
    "        except:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div')\n",
    "            try:\n",
    "                Set.append(set_clean(set_extractor(item_set.text)))\n",
    "            except:\n",
    "                Set.append(np.nan)   \n",
    "\n",
    "        set_length = len((set_clean(set_extractor(item_set.text))))\n",
    "\n",
    "        try:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div[2]')\n",
    "            try:\n",
    "                Encores.append(encore_clean(encore_extractor(item_set.text)))\n",
    "\n",
    "            except:\n",
    "                Encores.append(np.nan)\n",
    "\n",
    "        except:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div')\n",
    "            try:\n",
    "                Encores.append(encore_clean(encore_extractor(item_set.text)))\n",
    "            except:\n",
    "                Encores.append(np.nan) \n",
    "\n",
    "        encore_length = len(encore_clean(encore_extractor(item_set.text)))\n",
    "\n",
    "\n",
    "        try:\n",
    "            Set_Length.append(set_length+encore_length)\n",
    "        except:\n",
    "            Set_Length.append(np.nan)         \n",
    "\n",
    "        try:\n",
    "            Encores_Count.append(len(str(item_set.text).split('ENCORE'))-1)\n",
    "        except:\n",
    "            Encores_Count.append(np.nan)\n",
    "\n",
    "#=== final exception, if I've missed any elements this will return as NaN. Hopefully not too many.\n",
    "    except:\n",
    "        Set.append(np.nan)\n",
    "        Encores.append(np.nan)\n",
    "        Encores_Count.append(np.nan)\n",
    "        Set_Length.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Date':Date,\n",
    "            'Venue':Venue,\n",
    "            'City_Country':City_Country,\n",
    "            'Tour': Tour,\n",
    "            'Set': Set,\n",
    "            'Encores':Encores,\n",
    "            'Encores_Count' : Encores_Count,\n",
    "            'Set_Length':Set_Length,\n",
    "            'Other_Acts' : Other_Acts,                   \n",
    "            'URL': URL }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "# `Save final data`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Scraped Data/Metallica_Data_Dirty.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next\"></a>\n",
    "# `Next Steps`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay - so now we have a solid database containing all of Metallica's gigs to date. We have: \n",
    "\n",
    "<b>Date', 'Venue', 'City_Country', 'Tour', 'Set', 'Encores','Encores_Count', 'Set_Length', 'Other_Acts', 'URL'</b>\n",
    "\n",
    "The next step is to obtain our secondary data source - album data -  and overlay this with our live data in order to get a view of what albums are most likely to be heard live. We will also then want to use Googlemaps API in order to get geo-coded location data for each of the gigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
