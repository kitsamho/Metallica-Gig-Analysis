{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\">\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the structure of the web content on metallica.com and work out the best strategy to scrape the data we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations: \n",
    "\n",
    "There are c.100 pages that cover all gigs from 2018-1982. Each of these pages has its own unique URL. Let's call these 'page URLs'. \n",
    "    \n",
    "Each of these 100 page URL's link to a page containing links to 22 gigs (accessible by a 'more information' button on each of the 100 pages). \n",
    "\n",
    "Therefore it seems that the most efficient strategy is to first scrape the 100 page URLs and then for each page URL scrape the 21 gig URLS that are contained. The ultimate aim is to create a list of *all* gig urls - which should be about 2100 in total. \n",
    "\n",
    "Once we have all gig URLS we can run a script to scrape all the relevant gig data from them sequentially in a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining page URL's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for page URL's \n",
    "page_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise driver and scrape all the page URL's\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "#Get URL's for each page\n",
    "driver.get('https://www.metallica.com/tour/past/?sz=21&start=0')    \n",
    "page = driver.find_element_by_class_name('page-next')\n",
    "page_urls.append(page.get_attribute('href'))     \n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        driver.get(page_urls[-1])\n",
    "        page = driver.find_element_by_class_name('page-next')\n",
    "        page_urls.append(page.get_attribute('href'))    \n",
    "    except:\n",
    "        break\n",
    "\n",
    "page_urls.insert(0,'https://www.metallica.com/tour/past/?sz=21&start=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.metallica.com/tour/past/?sz=21&amp;sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Page URLs\n",
       "0  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "1  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "2  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "3  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "4  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "5  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "6  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "7  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "8  https://www.metallica.com/tour/past/?sz=21&sta...\n",
       "9  https://www.metallica.com/tour/past/?sz=21&sta..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks like it worked\n",
    "page_urls_df = pd.DataFrame(page_urls)\n",
    "page_urls_df.columns =['Page URLs']\n",
    "page_urls_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save these just in case we need them later\n",
    "page_urls_df.to_csv('Metallica_Page_URLS.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining gig URL's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for gig URL's \n",
    "gig_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the gig URL's on each page URL\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "for i in page_urls:\n",
    "    driver.get(i) \n",
    "    all_gigs = driver.find_elements_by_class_name('show')\n",
    "\n",
    "    for each in all_gigs:\n",
    "        show_id = each.get_attribute('data-show-id')\n",
    "        try:\n",
    "            gig_url.append(f'https://www.metallica.com/events/{show_id}.html')\n",
    "        except:\n",
    "            gig_url.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like it's worked\n",
    "gig_urls_df = pd.DataFrame(gig_url)\n",
    "gig_urls_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, let's save it to a CSV just in case we need it later on\n",
    "gig_urls_df.to_csv('Metallica_Gig_URLS',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up empty lists to contain the relevant data from each gig url\n",
    "Date = []\n",
    "Venue = []\n",
    "City_Country = []\n",
    "Tour = []\n",
    "Set = []\n",
    "Encores = []\n",
    "Number_of_Encores = []\n",
    "Set_Length = []\n",
    "URL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes numbers\n",
    "def removenumbers(x):\n",
    "    return re.sub(\"\\d\",\"\",x)\n",
    "\n",
    "#returns location of gig\n",
    "def location(x):\n",
    "    return x.split('\\n')[0]\n",
    "\n",
    "#returns gig venue\n",
    "def venue(x):\n",
    "    venue_date = x.split('\\n')[1]\n",
    "    return venue_date.split(' / ')[0]\n",
    "\n",
    "#returns gig date\n",
    "def date(x):\n",
    "    venue_date = x.split('\\n')[1]\n",
    "    return venue_date.split(' / ')[1]\n",
    "\n",
    "#returns tour name\n",
    "def tour_clean(x):\n",
    "    return x.split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts set \n",
    "def set_extractor(x):\n",
    "    clean = removenumbers(x)\n",
    "    output = clean.split('ENCORE')[0]\n",
    "    return output\n",
    "\n",
    "#cleans set text\n",
    "def set_clean(x):\n",
    "    new = x.split('\\n\\n')\n",
    "    return [i.replace('\\n','') for i in new][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts encore \n",
    "def encore_extractor(x):\n",
    "    clean = removenumbers(x)\n",
    "    output = clean.split('ENCORE')[1:]\n",
    "    return str(output)\n",
    "\n",
    "#cleans encore text\n",
    "def encore_clean(x):\n",
    "    clean = x.replace(' #','').replace(\"['\",'').replace(\"']\",'').replace(\"\\\\n', '\",'').split('\\\\n\\\\n')\n",
    "    return clean[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts tour\n",
    "def tour_finder(x):\n",
    "    try:\n",
    "        one = x.split('TOUR NAME')[1]\n",
    "        return one.split('\\n')[1]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts support acts\n",
    "def other_acts(x):\n",
    "    try:\n",
    "        one = x.split('OTHER ACTS')[1]\n",
    "        return one.split('\\n')[1]\n",
    "    except:\n",
    "        return np.nan\n",
    "       \n",
    "#if other acts are several, this splits them and appends the to a list\n",
    "def other_act_split(x):\n",
    "    try:\n",
    "        return x.split(',')\n",
    "    except:\n",
    "        return list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import gig URL's\n",
    "gig_urls = pd.read_csv('../1.1_Data_Acquisition_URLs/Metallica_Gig_URLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2070"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 2070 gigs to scrape\n",
    "len(gig_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gig urls to list\n",
    "all_gigs = list(gig_urls['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2988c4595b89482db405aa04ef8ea7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set up empty lists\n",
    "Date = []\n",
    "Venue = []\n",
    "City_Country = []\n",
    "Tour = []\n",
    "Set = []\n",
    "Encores = []\n",
    "Encores_Count = []\n",
    "Set_Length = []\n",
    "Other_Acts = []\n",
    "URL = []\n",
    "\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "for gig in tqdm_notebook(all_gigs):\n",
    "    driver.get(gig)\n",
    "\n",
    "#=== append URL, location and date data - some of the cleaning functions above built in so less to clean afterwards    \n",
    "#=== URL appending will be good for spot checks to see if any NaN's are geniune\n",
    "\n",
    "    URL.append(gig)\n",
    "    item_location_date = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[1]/div/div')\n",
    "    try:\n",
    "        City_Country.append(location(item_location_date.text).title())\n",
    "    except:\n",
    "        City_Country.append(np.nan)\n",
    "    try:\n",
    "        Date.append(date(item_location_date.text).title())\n",
    "    except:\n",
    "        Date.append(np.nan)\n",
    "    try:\n",
    "        Venue.append(venue(item_location_date.text).title())\n",
    "    except:\n",
    "        Venue.append(np.nan)\n",
    "\n",
    "#=== append Tour name data\n",
    "    item_content = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]')\n",
    "    try:\n",
    "        Tour.append(tour_finder(item_content.text))\n",
    "    except:\n",
    "        Tour.append(np.nan) \n",
    "\n",
    "#=== append other act data\n",
    "    item_content = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]')\n",
    "    try:\n",
    "        Other_Acts.append([i.strip() for i in other_act_split(other_acts(item_content.text))])\n",
    "    except:\n",
    "        Other_Acts.append(np.nan)   \n",
    "        \n",
    "\n",
    "#=== append Set and Encore data. There are several places on the page this appears across the 2000 gigs so need multiple elements\n",
    "    try:\n",
    "        try:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div[2]')\n",
    "            try:\n",
    "                Set.append(set_clean(set_extractor(item_set.text)))\n",
    "            except:\n",
    "                Set.append(np.nan)\n",
    "\n",
    "        except:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div')\n",
    "            try:\n",
    "                Set.append(set_clean(set_extractor(item_set.text)))\n",
    "            except:\n",
    "                Set.append(np.nan)   \n",
    "\n",
    "        set_length = len((set_clean(set_extractor(item_set.text))))\n",
    "\n",
    "        try:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div[2]')\n",
    "            try:\n",
    "                Encores.append(encore_clean(encore_extractor(item_set.text)))\n",
    "\n",
    "            except:\n",
    "                Encores.append(np.nan)\n",
    "\n",
    "        except:\n",
    "            item_set = driver.find_element_by_xpath('//*[@id=\"primary\"]/div[2]/div/div/div[1]/div[1]/div')\n",
    "            try:\n",
    "                Encores.append(encore_clean(encore_extractor(item_set.text)))\n",
    "            except:\n",
    "                Encores.append(np.nan) \n",
    "\n",
    "        encore_length = len(encore_clean(encore_extractor(item_set.text)))\n",
    "\n",
    "\n",
    "        try:\n",
    "            Set_Length.append(set_length+encore_length)\n",
    "        except:\n",
    "            Set_Length.append(np.nan)         \n",
    "\n",
    "        try:\n",
    "            Encores_Count.append(len(str(item_set.text).split('ENCORE'))-1)\n",
    "        except:\n",
    "            Encores_Count.append(np.nan)\n",
    "\n",
    "#=== final exception, if I've missed any elements this will return as NaN. Hopefully not too many.\n",
    "    except:\n",
    "        Set.append(np.nan)\n",
    "        Encores.append(np.nan)\n",
    "        Encores_Count.append(np.nan)\n",
    "        Set_Length.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that arrays are equal - they are, whoop whoop!\n",
    "a =[Date, Venue, City_Country, Tour, Set, Encores, Encores_Count, Set_Length,Other_Acts, URL]\n",
    "[len(i) for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Date':Date,\n",
    "            'Venue':Venue,\n",
    "            'City_Country':City_Country,\n",
    "            'Tour': Tour,\n",
    "            'Set': Set,\n",
    "            'Encores':Encores,\n",
    "            'Encores_Count' : Encores_Count,\n",
    "            'Set_Length':Set_Length,\n",
    "            'Other_Acts' : Other_Acts,                   \n",
    "            'URL': URL }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the DF to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Metallica_Data_Dirty',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
